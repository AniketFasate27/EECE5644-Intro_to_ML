{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # For plotting\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\anifa\\Desktop\\GITB\\EECE5644-Intro_to_ML\\Project\\Dataset\\processed_data_without_packet_loss_delay\\processed_data_without_packet_loss_delay.csv\")\n",
    "\n",
    "# Drop any columns that are completely empty or that are just unnamed indices\n",
    "df.drop(columns=[col for col in df.columns if 'Unnamed' in col or df[col].isna().all()], inplace=True)\n",
    "\n",
    "# Create a mapping from the combined numerical target to the specific slice type names\n",
    "class_mapping = {\n",
    "    0: 'Slice Type_URLLC',\n",
    "    1: 'Slice Type_eMBB',\n",
    "    2: 'Slice Type_mMTC'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the target column based on the max value across specific columns\n",
    "df['Slice Type (Output)'] = df[['Slice Type (Output)_URLLC', 'Slice Type (Output)_eMBB', 'Slice Type (Output)_mMTC']].idxmax(axis=1)\n",
    "df['Slice Type (Output)'] = df['Slice Type (Output)'].map({\n",
    "    'Slice Type (Output)_URLLC': class_mapping[0],\n",
    "    'Slice Type (Output)_eMBB': class_mapping[1],\n",
    "    'Slice Type (Output)_mMTC': class_mapping[2]\n",
    "})\n",
    "\n",
    "# Separate features and the combined target\n",
    "X = df.drop(['Slice Type (Output)_URLLC', 'Slice Type (Output)_eMBB', 'Slice Type (Output)_mMTC', 'Slice Type (Output)'], axis=1)\n",
    "y = df['Slice Type (Output)']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Use a subset of the data for RandomizedSearchCV\n",
    "subset_fraction = 0.1  # Using 10% of the data for faster execution\n",
    "subset_index = np.random.choice(X_train.shape[0], int(X_train.shape[0] * subset_fraction), replace=False)\n",
    "X_train_subset = X_train[subset_index]\n",
    "y_train_subset = y_train.iloc[subset_index]\n",
    "\n",
    "# Define the logistic regression model\n",
    "model = LogisticRegression(max_iter=10000, multi_class='ovr', tol=0.01)\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_distributions = {\n",
    "    'C': np.logspace(-3, 2, 6),\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to the subset of training data\n",
    "random_search.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_prob = best_model.predict_proba(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC AUC for each class\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "for i, label in enumerate(class_mapping.values()):\n",
    "    fpr[label], tpr[label], _ = roc_curve(y_test == label, y_prob[:, i])\n",
    "    roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, color in zip(class_mapping.values(), ['blue', 'red', 'green']):\n",
    "    plt.plot(fpr[label], tpr[label], color=color, label=f'ROC curve of {label} (area = {roc_auc[label]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multiclass ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy, F1 score, and generate confusion matrix and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=list(class_mapping.values()))\n",
    "class_report = classification_report(y_test, y_pred, labels=list(class_mapping.values()))\n",
    "\n",
    "print(f'Accuracy on Test set: {accuracy:.4f}')\n",
    "print(f'F1 Score (Weighted Average): {f1:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_mapping.values(), yticklabels=class_mapping.values())\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aniket",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
